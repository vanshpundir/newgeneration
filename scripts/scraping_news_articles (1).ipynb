{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping News Aritcles - Hindi Newspaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Scraping newspaper article from a newpaper website. A program that will navigate thorught the section of the newspaper and there pages to scrap news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jagaran Newspaper's official url\n",
    "url = \"https://zeenews.india.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to request a link\n",
    "def request_url(link):\n",
    "    \"\"\"\n",
    "    It takes a url and returns the html as string.\n",
    "    \"\"\"\n",
    "    ## Slow things down ## \n",
    "    ## Let the site breath ##\n",
    "    time.sleep(2)\n",
    "    \n",
    "    response = requests.get(link)\n",
    "    html = response.text\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse html\n",
    "def parse_html(to_parse):\n",
    "    \"\"\"\n",
    "    It takes a string, then parse it.\n",
    "    Finally, it retuns a soup object.\n",
    "    \"\"\"\n",
    "    soup = bs4.BeautifulSoup(to_parse, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/world/in-race-to-the-moon-gold-chandrayaan-3-moves-ahead-as-russian-luna-25-fails-to-enter-orbit-2651090.html\" title=\"Chandrayaan-3 Vs Luna-25: ISRO Heads Towards Big Success, Russia Faces Setback\"><img alt=\"Chandrayaan-3 Vs Luna-25: ISRO Heads Towards Big Success, Russia Faces Setback\" class=\"img-fluid lazyload\" data-src=\"https://english.cdn.zeenews.com/sites/default/files/styles/zm_700x400/public/2023/08/20/1266302-luna-vs-chandrayaan.jpg\" height=\"150\" src=\"https://english.cdn.zeenews.com/static/apprun/fallbackimages/zee_news_eng_fallback_optimized.webp\" width=\"250\"/></a>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to collect all sections links\n",
    "# Like world news, national news ...\n",
    "\n",
    "\n",
    "def all_section(main_url):\n",
    "    \"\"\"\n",
    "    It takes a main url of the newspaper and then\n",
    "    finds almost all the sections in the newspaper.\n",
    "    Finally, it returns the section which we will scrap.\n",
    "    \"\"\"\n",
    "    soup = parse_html(request_url(url))\n",
    "    ul = soup.find(\"div\", class_=\"medium-thumb-list\")\n",
    "#     section_list = []\n",
    "#     for li in ul.find_all(\"li\"):\n",
    "#         section_list.append(li.a.get('href'))\n",
    "#     # Remove the section which we will not consider\n",
    "#     # Like the video section and others\n",
    "#     remove = [0, 1, -1, -1, -1]\n",
    "#     for i in remove:\n",
    "#         section_list.remove(s+-ection_list[i])\n",
    "#     return section_list\n",
    "    return ul.a\n",
    "\n",
    "content = all_section(url)\n",
    "\n",
    "all_section(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/world/in-race-to-the-moon-gold-chandrayaan-3-moves-ahead-as-russian-luna-25-fails-to-enter-orbit-2651090.html\n"
     ]
    }
   ],
   "source": [
    "def href_section(main_url):\n",
    "    \"\"\"\n",
    "    It takes a main url of the newspaper and then\n",
    "    finds almost all the sections in the newspaper.\n",
    "    Finally, it returns the section which we will scrap.\n",
    "    \"\"\"\n",
    "    soup = parse_html(request_url(main_url))\n",
    "    ul = soup.find(\"div\", class_=\"medium-thumb-list\")\n",
    "    \n",
    "    if ul is None:\n",
    "        print(\"Could not find the section container.\")\n",
    "        return\n",
    "    \n",
    "    a_tags = ul.find_all(\"a\")  # Find all <a> tags within the <div>\n",
    "    \n",
    "    if not a_tags:\n",
    "        print(\"Could not find any <a> tags.\")\n",
    "        return\n",
    "    \n",
    "    for a_tag in a_tags:\n",
    "        title = a_tag.get(\"title\")\n",
    "        href = a_tag.get(\"href\")\n",
    "        \n",
    "        if title and href:\n",
    "            \n",
    "            return href\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "href = href_section(url)\n",
    "print(href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def description_section(main_url):\n",
    "    \"\"\"\n",
    "    It takes a main url of the newspaper and then\n",
    "    finds almost all the sections in the newspaper.\n",
    "    Finally, it returns the section which we will scrap.\n",
    "    \"\"\"\n",
    "    soup = parse_html(request_url(main_url))  # Fixed: Changed 'url' to 'main_url'\n",
    "    ul = soup.find(\"div\", class_=\"row\")\n",
    "   \n",
    "#     if ul is None:\n",
    "#         print(\"Could not find the section container.\")\n",
    "#         return\n",
    "    \n",
    "#     a_tag = ul.find(\"p\")  # Find the first <a> tag within the <div>\n",
    "    \n",
    "#     if a_tag is None:\n",
    "#         print(\"Could not find the <a> tag.\")\n",
    "#         return\n",
    "    \n",
    "#    description = a_tag.get(\"title\")  # Get the value of the \"title\" attribute\n",
    "    \n",
    "    \n",
    "    return ul \n",
    "\n",
    "print(description_section(url+href))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chandrayaan-3 Vs Luna-25: ISRO Heads Towards Big Success, Russia Faces Setback\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def title_section(main_url):\n",
    "    \"\"\"\n",
    "    It takes a main url of the newspaper and then\n",
    "    finds almost all the sections in the newspaper.\n",
    "    Finally, it returns the section which we will scrap.\n",
    "    \"\"\"\n",
    "    soup = parse_html(request_url(main_url))  # Fixed: Changed 'url' to 'main_url'\n",
    "    ul = soup.find(\"div\", class_=\"medium-thumb-list\")\n",
    "    \n",
    "    if ul is None:\n",
    "        print(\"Could not find the section container.\")\n",
    "        return\n",
    "    \n",
    "    a_tag = ul.find(\"a\")  # Find the first <a> tag within the <div>\n",
    "    \n",
    "    if a_tag is None:\n",
    "        print(\"Could not find the <a> tag.\")\n",
    "        return\n",
    "    \n",
    "    title = a_tag.get(\"title\")  # Get the value of the \"title\" attribute\n",
    "    \n",
    "    print(title)\n",
    "\n",
    "title_section(url)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Source: https://english.cdn.zeenews.com/sites/default/files/styles/zm_700x400/public/2023/08/20/1266302-luna-vs-chandrayaan.jpg\n"
     ]
    }
   ],
   "source": [
    "def image_section(main_url):\n",
    "    \"\"\"\n",
    "    It takes a main url of the newspaper and then\n",
    "    finds almost all the sections in the newspaper.\n",
    "    Finally, it returns the section which we will scrap.\n",
    "    \"\"\"\n",
    "    soup = parse_html(request_url(url))\n",
    "    div_with_sections = soup.find(\"div\", class_=\"medium-thumb-list\")\n",
    "    \n",
    "    if div_with_sections is None:\n",
    "        print(\"Could not find the section container.\")\n",
    "        return\n",
    "    \n",
    "    a_tag = div_with_sections.find(\"a\")  # Find the first <a> tag within the <div>\n",
    "    \n",
    "    if a_tag is None:\n",
    "        print(\"Could not find the <a> tag.\")\n",
    "        return\n",
    "    \n",
    "    img_tag = a_tag.find(\"img\")  # Find the <img> tag within the <a> tag\n",
    "    \n",
    "    if img_tag is None:\n",
    "        print(\"Could not find the <img> tag.\")\n",
    "        return\n",
    "    \n",
    "    image_source = img_tag.get(\"data-src\")  # Get the value of the \"data-src\" attribute\n",
    "    \n",
    "    print(\"Image Source:\", image_source)\n",
    "\n",
    "all_section(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_section(main_url):\n",
    "    soup = parse_html(request_url(main_url))\n",
    "    div_with_sections = soup.find(\"div\", class_=\"medium-thumb-list\")\n",
    "    \n",
    "    if div_with_sections is None:\n",
    "        print(\"Could not find the section container.\")\n",
    "        return []\n",
    "    \n",
    "    ul = div_with_sections.ul\n",
    "    \n",
    "    if ul is None:\n",
    "        print(\"Could not find the list of sections.\")\n",
    "        return []\n",
    "    \n",
    "    section_list = []\n",
    "    for li in ul.find_all(\"li\"):\n",
    "        section_list.append(li.a.get('href'))\n",
    "    \n",
    "    remove = [0, 1, -1, -1, -1]\n",
    "    for i in remove:\n",
    "        if 0 <= i < len(section_list):\n",
    "            section_list.pop(i)\n",
    "    \n",
    "    return section_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the section container.\n"
     ]
    }
   ],
   "source": [
    "# All section url (half-urls)\n",
    "section_urls = all_section(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the sections urls\n",
    "section_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orientation of the section pages**\n",
    "\n",
    "In this newspaper there are basically two types of layouts: Grid View and List View. So we will filter the section of the grid and list view seperately. As depending upon the layouts the html fromat cahges, therefore we will have to scrap them differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep url of the section which has Grid layout\n",
    "grid_layout_urls = [3, 4, 6, 7, 8, 9]\n",
    "\n",
    "# All Grid half urls\n",
    "grid_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep url of the section, which has linear layout(List) view\n",
    "list_layout_urls = [0, 1, 5]\n",
    "\n",
    "# All List half urls\n",
    "list_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only Gride View section page urls\n",
    "for filter_url in grid_layout_urls:\n",
    "    grid_urls.append(section_urls[filter_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only List View section page urls\n",
    "for filter_url in list_layout_urls:\n",
    "    list_urls.append(section_urls[filter_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/politics-news-hindi.html',\n",
       " '/world-news-hindi.html',\n",
       " '/technology-hindi.html',\n",
       " '/business-hindi.html',\n",
       " '/cricket-hindi.html',\n",
       " '/automobile']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Grid View urls\n",
    "grid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/latest-news.html',\n",
       " '/news/national-news-hindi.html',\n",
       " '/common-man-issue-news-hindi.html']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking List view urls\n",
    "list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build complete url\n",
    "# Returns a string\n",
    "def complete_url(half_url):\n",
    "    \"\"\"\n",
    "    This takes a second half of the url as input\n",
    "    and then it adds the first part of the url.\n",
    "    Finally it returns a complete url.\n",
    "    \"\"\"\n",
    "   # Join the url with the href of world news\n",
    "    full_url = url + half_url\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store comple urls of the Grid View\n",
    "final_grid_urls =[]\n",
    "\n",
    "# To store comple urls of the List View\n",
    "final_list_urls =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting half urls to complete urls - Grid View\n",
    "for url_get in grid_urls:\n",
    "    final_grid_urls.append(complete_url(url_get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting half urls to complete urls - List View\n",
    "for url_set in list_urls:\n",
    "    final_list_urls.append(complete_url(url_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.jagran.com/politics-news-hindi.html',\n",
       " 'https://www.jagran.com/world-news-hindi.html',\n",
       " 'https://www.jagran.com/technology-hindi.html',\n",
       " 'https://www.jagran.com/business-hindi.html',\n",
       " 'https://www.jagran.com/cricket-hindi.html',\n",
       " 'https://www.jagran.com/automobile']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the full urls - Grid View\n",
    "final_grid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.jagran.com/latest-news.html',\n",
       " 'https://www.jagran.com/news/national-news-hindi.html',\n",
       " 'https://www.jagran.com/common-man-issue-news-hindi.html']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the full urls - List View\n",
    "final_list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for valid urls\n",
    "def valid_url(url):\n",
    "    \"\"\"\n",
    "    Takes an url and checks if the urls is valid.\n",
    "    Returns a boolearn value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        urllib2.urlopen(url)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect article uls only for Grid view sections\n",
    "def collect(page_urls):\n",
    "    \"\"\"\n",
    "    Takes a list of urls which has a grid view layout,\n",
    "    then it extracts the urls of the articles from it\n",
    "    and then it returns it.\n",
    "    \"\"\"\n",
    "    print(\"Extracting article urls from the following sections:\")\n",
    "    \n",
    "    all_urls = set()\n",
    "    for page_url in page_urls:\n",
    "        print(page_url)\n",
    "        soup_page = parse_html(request_url(page_url))\n",
    "        for div in soup_page.find_all(class_=\"h3\"):\n",
    "            sec_head_href = div.find(\"a\").get(\"href\")\n",
    "            # Checks if the url is valid\n",
    "            # Add only if the url is valid\n",
    "            if valid_url(sec_head_href):\n",
    "                all_urls.add(sec_head_href)\n",
    "            else:\n",
    "                all_urls.add(complete_url(sec_head_href))\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting article urls from the following sections:\n",
      "https://www.jagran.com/politics-news-hindi.html\n",
      "https://www.jagran.com/world-news-hindi.html\n",
      "https://www.jagran.com/technology-hindi.html\n",
      "https://www.jagran.com/business-hindi.html\n",
      "https://www.jagran.com/cricket-hindi.html\n",
      "https://www.jagran.com/automobile\n"
     ]
    }
   ],
   "source": [
    "# Function call to collect all article urls from Grid View Sections\n",
    "all_urls = collect(final_grid_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of uniques aritcles urls\n",
    "# Uncomment the below line to check the length\n",
    "# len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects the article urls of the List view sections\n",
    "def linear_layout_page(linear_url_list):\n",
    "    \"\"\"\n",
    "    It takes urls of the list view sections and\n",
    "    extracts the article links, then it returns it.\n",
    "    \"\"\"\n",
    "    for url in linear_url_list:\n",
    "        soup = parse_html(request_url(url))\n",
    "        ul = soup.find(\"div\", class_=\"newsFJagran\").ul\n",
    "\n",
    "        for li in ul.find_all(\"li\"):\n",
    "            # Adding to the existing urls from the Grid View sections\n",
    "            all_urls.add(complete_url(li.a.get('href')))\n",
    "        \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion call to collect all the article urls - List View Sections\n",
    "all_urls = linear_layout_page(final_list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count - after adding articles urls from list view sections\n",
    "# len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes list of urls\n",
    "# Returns set with new set of urls\n",
    "def navigator(navigate_url):\n",
    "    \"\"\"\n",
    "    It takes urls of the List View sections and then navigates \n",
    "    to next page till the 10th page, along with it, it also add the page's url\n",
    "    to list. Finally, it returns the list of urls of the pages.\n",
    "    \"\"\"\n",
    "    next_page_url = []\n",
    "    next_page_set = set()\n",
    "    for navigate in navigate_url:\n",
    "        soup = parse_html(request_url(navigate))\n",
    "        url_class =  soup.find(class_=\"last\")\n",
    "        page_nav = url_class.a.get(\"href\")\n",
    "        page_nav = complete_url(page_nav)\n",
    "        next_page_url = page_nav\n",
    "        for _ in range(10):\n",
    "            soup_next = parse_html(request_url(next_page_url))\n",
    "            url_class_next =  soup_next.find(class_=\"last\")\n",
    "            page_nav_next = url_class_next.a.get(\"href\")\n",
    "            page_nav_next = complete_url(page_nav_next)\n",
    "            next_page_url = page_nav_next\n",
    "            next_page_set.add(next_page_url)\n",
    "    return next_page_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function call to collect the links of different pages of the sections - List View sections only\n",
    "aditional_url_set = navigator(final_list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the length of the adtional urls from the pages of sections\n",
    "# len(aditional_url_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the set into list\n",
    "aditional_url_list = list(aditional_url_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting article lists from the additional urls list\n",
    "all_urls = linear_layout_page(aditional_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of all urls\n",
    "# len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To be used for text retivring texts of articles\n",
    "article_urls_list = list(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_urls = pd.DataFrame(article_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the urls to CSV file\n",
    "df_final_urls.to_csv(\"final_urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract article text\n",
    "\n",
    "def article_text(article_urls):\n",
    "    \"\"\"\n",
    "    It takes article urls list and scrap the \n",
    "    texts from it. Finally, it returns the text\n",
    "    of the articles.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    for i in article_urls:\n",
    "        article_soup = parse_html(request_url(i))\n",
    "        div = article_soup.find(\"div\", class_=\"articleBody\")\n",
    "        for child_div in div.find_all(\"div\"):\n",
    "            child_div.decompose()\n",
    "        text.append(div.get_text())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** *The below cell will take more than 16 minitus to execute.*\n",
    "\n",
    "As we have to let the Jagran newspaper breath. We have given a delay of 2 seconds for extracting a article. So to extract 500 articles it will take atleast 1000 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(article_urls_list) > 500:\n",
    "    article_text_list = article_text(article_urls_list[0:500])\n",
    "else:\n",
    "    article_text_list = article_text(article_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(article_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv(\"articles_500.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Atlest 500 aritcle has been extracted from the Jagaran Newspaper with a crawler and exported into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html(html):\n",
    "    return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "def request_url(url):\n",
    "    response = requests.get(url)\n",
    "    return response.content\n",
    "\n",
    "def scrape_news(main_url, section_url):\n",
    "    full_section_url = f\"{main_url}{section_url}\"\n",
    "    html_content = request_url(full_section_url)\n",
    "    soup = parse_html(html_content)\n",
    "    \n",
    "    news_list = []\n",
    "    news_elements = soup.find_all(\"a\", class_=\"news-link\")\n",
    "    \n",
    "    for news_elem in news_elements:\n",
    "        news_title = news_elem.get_text()\n",
    "        news_link = news_elem.get(\"href\")\n",
    "        news_list.append({\"title\": news_title, \"link\": news_link})\n",
    "    \n",
    "    return news_list\n",
    "\n",
    "# Example usage\n",
    "main_url = \"https://zeenews.india.com/\"\n",
    "section_url = \"https://zeenews.india.com/auto/honda-elevate-to-launch-in-india-on-september-4-heres-all-you-need-to-know-2651092.html\"  # Replace with the actual section URL\n",
    "news_data = scrape_news(main_url, section_url)\n",
    "\n",
    "for news_item in news_data:\n",
    "    print(news_item[\"title\"])\n",
    "    print(news_item[\"link\"])\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\program files\\python311\\lib\\site-packages (4.11.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\program files\\python311\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install beautifulsoup4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
